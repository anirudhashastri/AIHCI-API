{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access API key\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Groq client instance\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Content: Let's chat about two exciting topics in the world of AI: the LLaMA 3.70B model and the Groq API!\n",
      "\n",
      "**Me:** Hey there! Have you heard about the LLaMA 3.70B model?\n",
      "\n",
      "**You:** Hmm, no, I don't think so. What's that?\n",
      "\n",
      "**Me:** LLaMA is a large language model developed by Meta AI. The 3.70B variant is a specific version of the model that's been trained on a massive dataset of text from the internet. It's capable of understanding and generating human-like language, which is pretty cool!\n",
      "\n",
      "**You:** Wow, that sounds impressive! What kind of tasks can it handle?\n",
      "\n",
      "**Me:** LLaMA 3.70B can do a lot of things, like conversing with users, answering questions, generating text, and even creating stories or dialogues. It's also great at handling multiple topics and responding to follow-up questions. The possibilities are endless!\n",
      "\n",
      "**You:** That's amazing. I can see how it could be useful for chatbots, customer service, and more. What about the Groq API? How does that fit into the picture?\n",
      "\n",
      "**Me:** Ah, excellent question! The Groq API is a cloud-based platform that allows developers to easily access and utilize models like LLaMA 3.70B. Think of it as a bridge that connects the model to your application or service.\n",
      "\n",
      "**You:** Okay, got it. So, I can use the Groq API to tap into the LLaMA model and integrate its capabilities into my own projects?\n",
      "\n",
      "**Me:** Exactly! With the Groq API, you can request responses from the LLaMA model, specify the input prompts, and even customize the output to fit your specific use case. It's a convenient way to leverage the power of large language models without having to worry about the underlying infrastructure or fine-tuning the model yourself.\n",
      "\n",
      "**You:** That sounds incredibly convenient. Are there any limitations or pricing considerations I should be aware of?\n",
      "\n",
      "**Me:** Great question! Depending on your use case and the volume of requests, there might be pricing implications. But don't worry, the Groq API is designed to be affordable and scalable. You can check the pricing details on the official website. As for limitations, the Groq API has some guidelines and rules in place to ensure responsible AI usage, so be sure to review those before getting started.\n",
      "\n",
      "**You:** Alright, I think I've got it! The LLaMA 3.70B model is a powerful language model, and the Groq API makes it easy to integrate its capabilities into my projects. Thanks for the intro!\n",
      "\n",
      "**Me:** You're welcome! It's an exciting time for AI innovation, and I'm happy to have helped you get started with these cutting-edge technologies.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    # Provide the prompt\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # \"content\": \"Introduce lamma3.70b model and groq API and give the answer the way yoda talk!\",\n",
    "            \"content\": \"Introduce lamma3.70b model and groq API in a conversational way! between yoda and obiwan\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "# Extract the generated content\n",
    "response_text = chat_completion.choices[0].message.content\n",
    "print(\"Generated Content:\", response_text)\n",
    "\n",
    "# Use TTS to speak the response\n",
    "engine.say(response_text)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HCIAPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
